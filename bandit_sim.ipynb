{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/bandit_simulator/blob/main/bandit_sim.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Callable, Optional, Union\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "np.random.seed(None)  # Use system entropy for true randomness\n",
    "random.seed(None)  # Use system entropy for true randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ BANDIT ENVIRONMENT CLASSES ================\n",
    "\n",
    "class BanditEnvironment:\n",
    "    \"\"\"Base class for bandit environments with 2 arms.\"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the bandit environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns in the game\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.current_turn = 0\n",
    "        self.p1 = None  # Probability of arm 1 (visible)\n",
    "        self.p2 = None  # Probability of arm 2 (hidden)\n",
    "        self.history = {\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'p1': [],\n",
    "            'p2': []\n",
    "        }\n",
    "    \n",
    "    def reset(self, T: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Reset the environment for a new game.\n",
    "        \n",
    "        Args:\n",
    "            T (int, optional): Number of turns for the new game\n",
    "        \"\"\"\n",
    "        if T is not None:\n",
    "            self.T = T\n",
    "        self.current_turn = 0\n",
    "        self.history = {\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'p1': [],\n",
    "            'p2': []\n",
    "        }\n",
    "        self._initialize_probabilities()\n",
    "    \n",
    "    def _initialize_probabilities(self) -> None:\n",
    "        \"\"\"Initialize the probabilities for both arms.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _update_probabilities(self) -> None:\n",
    "        \"\"\"Update the probabilities based on the current turn.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def step(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Take a step in the environment by selecting an arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        if self.current_turn >= self.T:\n",
    "            raise ValueError(\"Game is over, please reset.\")\n",
    "        \n",
    "        self._update_probabilities()\n",
    "        \n",
    "        # Record the current probabilities\n",
    "        self.history['p1'].append(self.p1)\n",
    "        self.history['p2'].append(self.p2)\n",
    "        \n",
    "        # Determine reward based on action\n",
    "        if action == 0:  # Arm 1\n",
    "            reward = 1.0 if np.random.random() < self.p1 else 0.0\n",
    "        else:  # Arm 2\n",
    "            reward = 1.0 if np.random.random() < self.p2 else 0.0\n",
    "        \n",
    "        # Update history\n",
    "        self.history['actions'].append(action)\n",
    "        self.history['rewards'].append(reward)\n",
    "        \n",
    "        # Increment turn\n",
    "        self.current_turn += 1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'current_turn': self.current_turn,\n",
    "            'total_turns': self.T,\n",
    "            'p1': self.p1,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "\n",
    "class FixedBandit(BanditEnvironment):\n",
    "    \"\"\"Bandit with fixed probabilities throughout the game.\"\"\"\n",
    "    \n",
    "    def _initialize_probabilities(self) -> None:\n",
    "        \"\"\"Initialize fixed probabilities for both arms.\"\"\"\n",
    "        self.p1 = np.random.uniform(0.01, 0.99)\n",
    "        self.p2 = np.random.uniform(0.01, 0.99)\n",
    "    \n",
    "    def _update_probabilities(self) -> None:\n",
    "        \"\"\"No updates for fixed bandits.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PeriodicBandit(BanditEnvironment):\n",
    "    \"\"\"Bandit with probabilities that change every k turns.\"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, k: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the periodic bandit.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            k (int): Frequency of probability changes\n",
    "        \"\"\"\n",
    "        super().__init__(T)\n",
    "        self.k = k\n",
    "    \n",
    "    def _initialize_probabilities(self) -> None:\n",
    "        \"\"\"Initialize starting probabilities.\"\"\"\n",
    "        self.p1 = np.random.uniform(0.01, 0.99)\n",
    "        self.p2 = np.random.uniform(0.01, 0.99)\n",
    "    \n",
    "    def _update_probabilities(self) -> None:\n",
    "        \"\"\"Update probabilities every k turns with completely new random values.\"\"\"\n",
    "        if self.current_turn % self.k == 0:\n",
    "            self.p1 = np.random.uniform(0.01, 0.99)\n",
    "            self.p2 = np.random.uniform(0.01, 0.99)\n",
    "\n",
    "\n",
    "class DynamicBandit(BanditEnvironment):\n",
    "    \"\"\"Bandit with probabilities that change every turn.\"\"\"\n",
    "    \n",
    "    def _initialize_probabilities(self) -> None:\n",
    "        \"\"\"Initialize starting probabilities.\"\"\"\n",
    "        self.p1 = np.random.uniform(0.01, 0.99)\n",
    "        self.p2 = np.random.uniform(0.01, 0.99)\n",
    "    \n",
    "    def _update_probabilities(self) -> None:\n",
    "        \"\"\"Update probabilities every turn with completely new random values.\"\"\"\n",
    "        # Completely random new values\n",
    "        self.p1 = np.random.uniform(0.01, 0.99)\n",
    "        self.p2 = np.random.uniform(0.01, 0.99)\n",
    "\n",
    "\n",
    "class FullyRandomBandit(BanditEnvironment):\n",
    "    \"\"\"Bandit where both arms are equally likely to be optimal, with random probabilities.\"\"\"\n",
    "    \n",
    "    def _initialize_probabilities(self) -> None:\n",
    "        \"\"\"Initialize probabilities randomly for both arms.\"\"\"\n",
    "        self.p1 = np.random.uniform(0.01, 0.99)\n",
    "        self.p2 = np.random.uniform(0.01, 0.99)\n",
    "    \n",
    "    def _update_probabilities(self) -> None:\n",
    "        \"\"\"\n",
    "        Randomly reassign probabilities at certain turns to ensure\n",
    "        no arm has a consistent advantage.\n",
    "        \"\"\"\n",
    "        # Randomly reassign probabilities based on random chance\n",
    "        if np.random.random() < 0.05:  # 5% chance per turn\n",
    "            self.p1 = np.random.uniform(0.01, 0.99)\n",
    "            self.p2 = np.random.uniform(0.01, 0.99)\n",
    "\n",
    "\n",
    "# ================ EXPERIMENT RUNNER ================\n",
    "\n",
    "def run_experiment(\n",
    "    agent_func: Callable,\n",
    "    n_games: int = 50,\n",
    "    default_turns: int = 100,\n",
    "    random_turns: bool = False,\n",
    "    verbose: bool = True,\n",
    "    env_type: str = \"all\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run an experiment with the given agent on all bandit environments.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to use\n",
    "        n_games (int): Number of games to play per environment\n",
    "        default_turns (int): Default number of turns per game (used when random_turns=False)\n",
    "        random_turns (bool): Whether to use random number of turns\n",
    "        verbose (bool): Whether to show progress bar\n",
    "        env_type (str): Type of environment to use (\"all\" or specific type)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Force different random seed each time\n",
    "    current_time = time.time()\n",
    "    np.random.seed(int(current_time * 1000) % 10000)\n",
    "    random.seed(int(current_time * 2000) % 10000)\n",
    "    \n",
    "    # Debug output\n",
    "    if random_turns:\n",
    "        print(\"RANDOM TURNS MODE: Will use different turn counts between 1-300 for each game\")\n",
    "        # Show some sample random turn values\n",
    "        samples = [np.random.randint(1, 301) for _ in range(5)]\n",
    "        print(f\"Sample turn counts: {samples}\")\n",
    "    else:\n",
    "        print(f\"FIXED TURNS MODE: Using T={default_turns} for all games\")\n",
    "    \n",
    "    all_environments = {\n",
    "        \"Fixed\": FixedBandit(),\n",
    "        \"Periodic\": PeriodicBandit(k=10),\n",
    "        \"Dynamic\": DynamicBandit(),\n",
    "        \"FullyRandom\": FullyRandomBandit()\n",
    "    }\n",
    "    \n",
    "    # If a specific environment type is requested, use only that one\n",
    "    if env_type != \"all\" and env_type in all_environments:\n",
    "        environments = [all_environments[env_type]]\n",
    "        environment_names = [env_type]\n",
    "    else:\n",
    "        # Otherwise use all environments\n",
    "        environments = list(all_environments.values())\n",
    "        environment_names = list(all_environments.keys())\n",
    "    \n",
    "    results = {\n",
    "        \"environment\": [],\n",
    "        \"game\": [],\n",
    "        \"total_reward\": [],\n",
    "        \"average_reward\": [],\n",
    "        \"turns\": [],\n",
    "        \"actions\": [],\n",
    "        \"optimal_actions\": [],\n",
    "        \"regret\": []\n",
    "    }\n",
    "    \n",
    "    # Create turn counts in advance for reproducibility and debugging\n",
    "    all_turn_counts = []\n",
    "    if random_turns:\n",
    "        for _ in range(n_games * len(environments)):\n",
    "            # Generate a truly random T between 1 and 300\n",
    "            all_turn_counts.append(np.random.randint(1, 301))\n",
    "    else:\n",
    "        all_turn_counts = [default_turns] * (n_games * len(environments))\n",
    "    \n",
    "    # Use tqdm for progress if verbose\n",
    "    game_iterator = tqdm(range(n_games * len(environments))) if verbose else range(n_games * len(environments))\n",
    "    \n",
    "    for game_idx in game_iterator:\n",
    "        env_idx = game_idx // n_games\n",
    "        game_num = game_idx % n_games\n",
    "        \n",
    "        env = environments[env_idx]\n",
    "        \n",
    "        # Get pre-generated turn count for this game\n",
    "        T = all_turn_counts[game_idx]\n",
    "        \n",
    "        # Debug output\n",
    "        if verbose and game_idx % 20 == 0:\n",
    "            print(f\"Game {game_idx}, Environment: {environment_names[env_idx]}, Turn count: {T}\")\n",
    "        \n",
    "        env.reset(T)\n",
    "        \n",
    "        total_reward = 0\n",
    "        optimal_actions = 0\n",
    "        \n",
    "        # Start the game\n",
    "        for _ in range(T):\n",
    "            # Prepare the visible info for the agent based on agent type\n",
    "            if agent_func == full_information_agent:\n",
    "                visible_info = env.get_visible_info()\n",
    "            elif agent_func == partial_information_agent:\n",
    "                visible_info = {\n",
    "                    'current_turn': env.current_turn,\n",
    "                    'total_turns': env.T,\n",
    "                    'p1': env.p1,\n",
    "                    'history': {\n",
    "                        'actions': env.history['actions'],\n",
    "                        'rewards': env.history['rewards']\n",
    "                    }\n",
    "                }\n",
    "            else:  # reward_only_agent\n",
    "                visible_info = {\n",
    "                    'current_turn': env.current_turn,\n",
    "                    'history': {\n",
    "                        'actions': env.history['actions'],\n",
    "                        'rewards': env.history['rewards']\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Get action from agent\n",
    "            action = agent_func(visible_info)\n",
    "            \n",
    "            # Take step in environment\n",
    "            reward = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Count optimal actions (choosing the arm with highest probability)\n",
    "            if (action == 0 and env.p1 >= env.p2) or (action == 1 and env.p2 > env.p1):\n",
    "                optimal_actions += 1\n",
    "        \n",
    "        # Calculate regret (difference between optimal and actual rewards)\n",
    "        optimal_expected_reward = sum(max(p1, p2) for p1, p2 in zip(env.history['p1'], env.history['p2']))\n",
    "        actual_expected_reward = sum(\n",
    "            env.history['p1'][i] if action == 0 else env.history['p2'][i]\n",
    "            for i, action in enumerate(env.history['actions'])\n",
    "        )\n",
    "        regret = optimal_expected_reward - actual_expected_reward\n",
    "        \n",
    "        # Store results\n",
    "        results[\"environment\"].append(environment_names[env_idx])\n",
    "        results[\"game\"].append(game_num)\n",
    "        results[\"total_reward\"].append(total_reward)\n",
    "        results[\"average_reward\"].append(total_reward / T)\n",
    "        results[\"turns\"].append(T)\n",
    "        results[\"actions\"].append(env.history['actions'])\n",
    "        results[\"optimal_actions\"].append(optimal_actions / T * 100)  # percentage\n",
    "        results[\"regret\"].append(regret)\n",
    "    \n",
    "    # Verify turn counts are as expected\n",
    "    if random_turns:\n",
    "        turn_counts = np.array(results[\"turns\"])\n",
    "        print(f\"\\nTurn count summary: Min={turn_counts.min()}, Max={turn_counts.max()}, Mean={turn_counts.mean():.2f}\")\n",
    "        if turn_counts.min() == turn_counts.max():\n",
    "            print(\"WARNING: All turn counts are the same! Random turns mode may not be working correctly.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ================ STANDARDIZED EXPERIMENT RUNNER ================\n",
    "\n",
    "def run_standard_experiment(agent_func: Callable, env_type: str, n_experiments: int = 100, fixed_turns: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a standardized experiment for a specific agent on a specific environment.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to use\n",
    "        env_type (str): Type of environment to use\n",
    "        n_experiments (int): Number of experiments to run (default 100)\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Force different random seed for this experiment\n",
    "    current_time = time.time()\n",
    "    np.random.seed(int(current_time * 1000) % 10000)\n",
    "    random.seed(int(current_time * 2000) % 10000)\n",
    "    \n",
    "    if fixed_turns:\n",
    "        print(\"\\n=== RUNNING WITH FIXED TURNS (T=100) ===\")\n",
    "        default_turns = 100\n",
    "        random_turns = False\n",
    "    else:\n",
    "        print(\"\\n=== RUNNING WITH RANDOM TURNS (T=1-300) ===\")\n",
    "        default_turns = 100  # This value is only used if random_turns is False\n",
    "        random_turns = True\n",
    "        \n",
    "        # Show some sample turn values to verify randomness\n",
    "        samples = [np.random.randint(1, 301) for _ in range(5)]\n",
    "        print(f\"Sample random turn values: {samples}\")\n",
    "    \n",
    "    # Run the experiment\n",
    "    return run_experiment(\n",
    "        agent_func=agent_func,\n",
    "        n_games=n_experiments,\n",
    "        default_turns=default_turns,\n",
    "        random_turns=random_turns,\n",
    "        verbose=True,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "\n",
    "# ================ VISUALIZATION FUNCTIONS ================\n",
    "\n",
    "def plot_rewards_by_environment(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot average rewards for each environment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Average Reward': results['average_reward']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df)\n",
    "    plt.title('Distribution of Average Rewards by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_optimal_actions(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot percentage of optimal actions for each environment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Optimal Actions (%)': results['optimal_actions']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Optimal Actions (%)', data=df)\n",
    "    plt.title('Percentage of Optimal Actions by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_regret(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot regret for each environment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Regret': results['regret']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Regret', data=df)\n",
    "    plt.title('Distribution of Regret by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_action_history(results: Dict, game_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot action history for a specific game.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        game_idx (int): Index of the game to plot\n",
    "    \"\"\"\n",
    "    env_name = results['environment'][game_idx]\n",
    "    actions = results['actions'][game_idx]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(actions, 'o-', markersize=4)\n",
    "    plt.yticks([0, 1], ['Arm 1', 'Arm 2'])\n",
    "    plt.title(f'Action History for {env_name} Environment - Game {results[\"game\"][game_idx]}')\n",
    "    plt.xlabel('Turn')\n",
    "    plt.ylabel('Action')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_reward_over_time(results: Dict, n_games: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Plot cumulative reward over time for selected games.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        n_games (int): Number of games to plot per environment\n",
    "    \"\"\"\n",
    "    # Select game indices to plot\n",
    "    env_names = list(set(results['environment']))\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for env_name in env_names:\n",
    "        env_game_indices = [i for i, e in enumerate(results['environment']) if e == env_name]\n",
    "        \n",
    "        # Select a subset of games for this environment\n",
    "        selected_indices = env_game_indices[:n_games]\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            actions = results['actions'][idx]\n",
    "            turns = results['turns'][idx]\n",
    "            \n",
    "            # Reconstruct cumulative rewards\n",
    "            cum_rewards = np.cumsum([results['total_reward'][idx] / turns] * turns)\n",
    "            \n",
    "            plt.plot(cum_rewards, alpha=0.7, label=f\"{env_name} - Game {results['game'][idx]}\")\n",
    "    \n",
    "    plt.title('Cumulative Reward Over Time')\n",
    "    plt.xlabel('Turn')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_statistical_summary(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive statistical summary from experiment results.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Statistical summary\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(results['environment'])):\n",
    "        data.append({\n",
    "            'Environment': results['environment'][i],\n",
    "            'Game': results['game'][i],\n",
    "            'Total Reward': results['total_reward'][i],\n",
    "            'Average Reward': results['average_reward'][i],\n",
    "            'Turns': results['turns'][i],\n",
    "            'Optimal Actions (%)': results['optimal_actions'][i],\n",
    "            'Regret': results['regret'][i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Group by environment and calculate statistics\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Total Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Average Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Optimal Actions (%)': ['mean', 'std', 'min', 'max'],\n",
    "        'Regret': ['mean', 'std', 'min', 'max'],\n",
    "        'Turns': ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def plot_performance_metrics(results: Dict, title_prefix: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Plot comprehensive performance metrics for the agent.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        title_prefix (str): Prefix for plot titles\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Average Reward': results['average_reward'],\n",
    "        'Optimal Actions (%)': results['optimal_actions'],\n",
    "        'Regret': results['regret'],\n",
    "        'Turns': results['turns']\n",
    "    })\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot average reward\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'{title_prefix}Average Reward by Environment')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimal action percentage\n",
    "    sns.boxplot(x='Environment', y='Optimal Actions (%)', data=df, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'{title_prefix}Optimal Actions (%) by Environment')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regret\n",
    "    sns.boxplot(x='Environment', y='Regret', data=df, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'{title_prefix}Regret by Environment')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning curve for each environment (using first few games)\n",
    "    for env_name in set(df['Environment']):\n",
    "        env_df = df[df['Environment'] == env_name]\n",
    "        \n",
    "        # Select a random game for this environment\n",
    "        selected_game = env_df.iloc[0]['Game']\n",
    "        \n",
    "        # Find the index in the original results\n",
    "        game_idx = next(i for i, (e, g) in enumerate(zip(results['environment'], results['game'])) \n",
    "                         if e == env_name and g == selected_game)\n",
    "        \n",
    "        actions = results['actions'][game_idx]\n",
    "        turns = results['turns'][game_idx]\n",
    "        \n",
    "        # Plot action history\n",
    "        axes[1, 1].plot(range(turns), actions, 'o-', markersize=3, label=env_name, alpha=0.7)\n",
    "    \n",
    "    axes[1, 1].set_title(f'{title_prefix}Action History by Environment')\n",
    "    axes[1, 1].set_xlabel('Turn')\n",
    "    axes[1, 1].set_ylabel('Action')\n",
    "    axes[1, 1].set_yticks([0, 1])\n",
    "    axes[1, 1].set_yticklabels(['Arm 1', 'Arm 2'])\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_simplified_results(results: Dict, agent_name: str = \"Agent\") -> None:\n",
    "    \"\"\"\n",
    "    Simplified visualization showing only aggregate results across all games.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        agent_name (str): Name of the agent for titles\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(results['environment'])):\n",
    "        data.append({\n",
    "            'Environment': results['environment'][i],\n",
    "            'Game': results['game'][i],\n",
    "            'Average Reward': results['average_reward'][i],\n",
    "            'Optimal Actions (%)': results['optimal_actions'][i],\n",
    "            'Regret': results['regret'][i],\n",
    "            'Turns': results['turns'][i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate and display statistical summary\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Average Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Optimal Actions (%)': ['mean', 'std', 'min', 'max'],\n",
    "        'Regret': ['mean', 'std'],\n",
    "        'Turns': ['mean', 'std', 'min', 'max', 'count']\n",
    "    })\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    # Print agent name and summary statistics\n",
    "    print(f\"\\n===== {agent_name} Summary Statistics =====\\n\")\n",
    "    summary_display = summary.round(3)  # Round to 3 decimal places for cleaner display\n",
    "    print(summary_display)\n",
    "    \n",
    "    # Print the turn count distribution to verify random generation is working\n",
    "    if \"Random\" in agent_name:\n",
    "        print(\"\\nTurn Count Distribution:\")\n",
    "        turn_counts = df.groupby('Turns').size().reset_index(name='Count')\n",
    "        turn_summary = f\"Min: {df['Turns'].min()}, Max: {df['Turns'].max()}, Mean: {df['Turns'].mean():.2f}, Std: {df['Turns'].std():.2f}\"\n",
    "        print(turn_summary)\n",
    "        \n",
    "        # Plot turn count histogram\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(df['Turns'], bins=30, alpha=0.7)\n",
    "        plt.title('Turn Count Distribution')\n",
    "        plt.xlabel('Number of Turns')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a figure with key metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Average Reward\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Environment', y='Average Reward', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Average Reward by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimal Actions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Environment', y='Optimal Actions (%)', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Optimal Actions (%) by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Environment', y='Regret', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Regret by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reward distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df)\n",
    "    plt.title(f'Reward Distribution by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{agent_name} Performance Across Environments\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show reward distributions as violin plots\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x='Environment', y='Average Reward', data=df)\n",
    "    plt.title(f'{agent_name}: Reward Distribution by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_results(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the results of the experiment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    # Convert results to pandas DataFrame for easier analysis\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Total Reward': results['total_reward'],\n",
    "        'Average Reward': results['average_reward'],\n",
    "        'Turns': results['turns'],\n",
    "        'Optimal Actions (%)': results['optimal_actions'],\n",
    "        'Regret': results['regret']\n",
    "    })\n",
    "    \n",
    "    # Summary statistics by environment\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Total Reward': ['mean', 'std'],\n",
    "        'Average Reward': ['mean', 'std'],\n",
    "        'Optimal Actions (%)': ['mean', 'std'],\n",
    "        'Regret': ['mean', 'std']\n",
    "    })\n",
    "    \n",
    "    print(\"===== Summary Statistics =====\")\n",
    "    print(summary)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Plot visualizations\n",
    "    plot_rewards_by_environment(results)\n",
    "    plot_optimal_actions(results)\n",
    "    plot_regret(results)\n",
    "    \n",
    "    # Plot action history for the first game of each environment\n",
    "    for env_name in ['Fixed', 'Periodic', 'Dynamic']:\n",
    "        game_idx = results['environment'].index(env_name)\n",
    "        plot_action_history(results, game_idx)\n",
    "    \n",
    "    # Plot reward over time for a subset of games\n",
    "    plot_reward_over_time(results, n_games=2)\n",
    "\n",
    "\n",
    "# ================ INDIVIDUAL AGENT EVALUATION FUNCTIONS ================\n",
    "\n",
    "def evaluate_full_information_agent(fixed_turns: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for just the full information agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the full information agent\n",
    "    \"\"\"\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    print(f\"===== EVALUATING FULL INFORMATION AGENT WITH {turns_type} TURNS =====\")\n",
    "    \n",
    "    # Dictionary to store results for each environment\n",
    "    results = {}\n",
    "    all_results = {}\n",
    "    \n",
    "    # Test on each environment type\n",
    "    env_types = [\"Fixed\", \"Periodic\", \"Dynamic\", \"FullyRandom\"]\n",
    "    for env_type in env_types:\n",
    "        print(f\"\\nTesting on {env_type} environment...\")\n",
    "        \n",
    "        # Run the experiment with specified turns type\n",
    "        env_results = run_standard_experiment(\n",
    "            agent_func=full_information_agent,\n",
    "            env_type=env_type,\n",
    "            fixed_turns=fixed_turns\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[env_type] = env_results\n",
    "        all_results.update({env_type: env_results})\n",
    "    \n",
    "    # Display aggregate results for all environments together\n",
    "    print(f\"\\n===== FULL INFORMATION AGENT SUMMARY ({turns_type} TURNS) =====\")\n",
    "    \n",
    "    # Combine all environment results\n",
    "    combined_results = {\n",
    "        \"environment\": [],\n",
    "        \"game\": [],\n",
    "        \"total_reward\": [],\n",
    "        \"average_reward\": [],\n",
    "        \"turns\": [],\n",
    "        \"actions\": [],\n",
    "        \"optimal_actions\": [],\n",
    "        \"regret\": []\n",
    "    }\n",
    "    \n",
    "    for env_results in results.values():\n",
    "        for key in combined_results.keys():\n",
    "            combined_results[key].extend(env_results[key])\n",
    "    \n",
    "    # Visualize the combined results\n",
    "    agent_name = f\"Full Information Agent ({turns_type})\"\n",
    "    visualize_simplified_results(combined_results, agent_name=agent_name)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def evaluate_partial_information_agent(fixed_turns: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for just the partial information agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the partial information agent\n",
    "    \"\"\"\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    print(f\"===== EVALUATING PARTIAL INFORMATION AGENT WITH {turns_type} TURNS =====\")\n",
    "    \n",
    "    # Dictionary to store results for each environment\n",
    "    results = {}\n",
    "    all_results = {}\n",
    "    \n",
    "    # Test on each environment type\n",
    "    env_types = [\"Fixed\", \"Periodic\", \"Dynamic\", \"FullyRandom\"]\n",
    "    for env_type in env_types:\n",
    "        print(f\"\\nTesting on {env_type} environment...\")\n",
    "        \n",
    "        # Run the experiment with specified turns type\n",
    "        env_results = run_standard_experiment(\n",
    "            agent_func=partial_information_agent,\n",
    "            env_type=env_type,\n",
    "            fixed_turns=fixed_turns\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[env_type] = env_results\n",
    "        all_results.update({env_type: env_results})\n",
    "    \n",
    "    # Display aggregate results for all environments together\n",
    "    print(f\"\\n===== PARTIAL INFORMATION AGENT SUMMARY ({turns_type} TURNS) =====\")\n",
    "    \n",
    "    # Combine all environment results\n",
    "    combined_results = {\n",
    "        \"environment\": [],\n",
    "        \"game\": [],\n",
    "        \"total_reward\": [],\n",
    "        \"average_reward\": [],\n",
    "        \"turns\": [],\n",
    "        \"actions\": [],\n",
    "        \"optimal_actions\": [],\n",
    "        \"regret\": []\n",
    "    }\n",
    "    \n",
    "    for env_results in results.values():\n",
    "        for key in combined_results.keys():\n",
    "            combined_results[key].extend(env_results[key])\n",
    "    \n",
    "    # Visualize the combined results\n",
    "    agent_name = f\"Partial Information Agent ({turns_type})\"\n",
    "    visualize_simplified_results(combined_results, agent_name=agent_name)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def evaluate_reward_only_agent(fixed_turns: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for just the reward-only agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the reward-only agent\n",
    "    \"\"\"\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    print(f\"===== EVALUATING REWARD-ONLY AGENT WITH {turns_type} TURNS =====\")\n",
    "    \n",
    "    # Dictionary to store results for each environment\n",
    "    results = {}\n",
    "    all_results = {}\n",
    "    \n",
    "    # Test on each environment type\n",
    "    env_types = [\"Fixed\", \"Periodic\", \"Dynamic\", \"FullyRandom\"]\n",
    "    for env_type in env_types:\n",
    "        print(f\"\\nTesting on {env_type} environment...\")\n",
    "        \n",
    "        # Run the experiment with specified turns type\n",
    "        env_results = run_standard_experiment(\n",
    "            agent_func=reward_only_agent,\n",
    "            env_type=env_type,\n",
    "            fixed_turns=fixed_turns\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[env_type] = env_results\n",
    "        all_results.update({env_type: env_results})\n",
    "    \n",
    "    # Display aggregate results for all environments together\n",
    "    print(f\"\\n===== REWARD-ONLY AGENT SUMMARY ({turns_type} TURNS) =====\")\n",
    "    \n",
    "    # Combine all environment results\n",
    "    combined_results = {\n",
    "        \"environment\": [],\n",
    "        \"game\": [],\n",
    "        \"total_reward\": [],\n",
    "        \"average_reward\": [],\n",
    "        \"turns\": [],\n",
    "        \"actions\": [],\n",
    "        \"optimal_actions\": [],\n",
    "        \"regret\": []\n",
    "    }\n",
    "    \n",
    "    for env_results in results.values():\n",
    "        for key in combined_results.keys():\n",
    "            combined_results[key].extend(env_results[key])\n",
    "    \n",
    "    # Visualize the combined results\n",
    "    agent_name = f\"Reward-Only Agent ({turns_type})\"\n",
    "    visualize_simplified_results(combined_results, agent_name=agent_name)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def evaluate_agent(agent_func: Callable, agent_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensively evaluate an agent across all environment types.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to evaluate\n",
    "        agent_name (str): Name of the agent for reporting\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all environments\n",
    "    \"\"\"\n",
    "    print(f\"===== Evaluating {agent_name} =====\")\n",
    "    \n",
    "    # Dictionary to store results for each environment\n",
    "    all_env_results = {}\n",
    "    \n",
    "    # Test on each environment type\n",
    "    env_types = [\"Fixed\", \"Periodic\", \"Dynamic\", \"FullyRandom\"]\n",
    "    for env_type in env_types:\n",
    "        print(f\"\\nTesting on {env_type} environment...\")\n",
    "        \n",
    "        # Fixed turns (T=100) for environments where T is known\n",
    "        results = run_standard_experiment(\n",
    "            agent_func=agent_func,\n",
    "            env_type=env_type,\n",
    "            fixed_turns=True  # Use T=100\n",
    "        )\n",
    "        \n",
    "        all_env_results[env_type] = results\n",
    "    \n",
    "    # If this is the reward-only agent, also test with unknown T\n",
    "    if agent_func == reward_only_agent:\n",
    "        print(\"\\nTesting reward-only agent with unknown number of turns...\")\n",
    "        \n",
    "        for env_type in env_types:\n",
    "            random_results = run_standard_experiment(\n",
    "                agent_func=agent_func,\n",
    "                env_type=env_type,\n",
    "                fixed_turns=False  # Random T between 1-300\n",
    "            )\n",
    "            all_env_results[f\"{env_type}_random_T\"] = random_results\n",
    "    \n",
    "    return all_env_results\n",
    "\n",
    "\n",
    "def run_all_agents() -> Dict:\n",
    "    \"\"\"\n",
    "    Run standard evaluations for all agent types.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Comprehensive results for all agents\n",
    "    \"\"\"\n",
    "    print(\"======= RUNNING COMPREHENSIVE AGENT EVALUATION =======\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each agent type\n",
    "    results[\"full_information\"] = evaluate_agent(full_information_agent, \"Full Information Agent\")\n",
    "    results[\"partial_information\"] = evaluate_agent(partial_information_agent, \"Partial Information Agent\")\n",
    "    results[\"reward_only\"] = evaluate_agent(reward_only_agent, \"Reward-Only Agent\")\n",
    "    \n",
    "    # Compare agents\n",
    "    print(\"\\n======= AGENT COMPARISON =======\")\n",
    "    compare_all_agents(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_all_agents(all_results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Compare the performance of all agents across environments.\n",
    "    \n",
    "    Args:\n",
    "        all_results (Dict): Results from run_all_agents\n",
    "    \"\"\"\n",
    "    # Prepare data for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Collect results from each agent and environment\n",
    "    for agent_name, agent_results in all_results.items():\n",
    "        for env_name, results in agent_results.items():\n",
    "            # Skip random T results for cleaner comparison\n",
    "            if \"_random_T\" in env_name:\n",
    "                continue\n",
    "                \n",
    "            # Calculate mean performance metrics for this agent in this environment\n",
    "            df = pd.DataFrame({\n",
    "                'Environment': results['environment'],\n",
    "                'Average Reward': results['average_reward'],\n",
    "                'Optimal Actions (%)': results['optimal_actions'],\n",
    "                'Regret': results['regret']\n",
    "            })\n",
    "            \n",
    "            # Get mean values\n",
    "            mean_reward = df['Average Reward'].mean()\n",
    "            mean_optimal = df['Optimal Actions (%)'].mean()\n",
    "            mean_regret = df['Regret'].mean()\n",
    "            \n",
    "            # Add to comparison data\n",
    "            comparison_data.append({\n",
    "                'Agent': agent_name.replace('_', ' ').title(),\n",
    "                'Environment': env_name,\n",
    "                'Average Reward': mean_reward,\n",
    "                'Optimal Actions (%)': mean_optimal,\n",
    "                'Regret': mean_regret\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display comprehensive summary table (rounded to 3 decimal places for readability)\n",
    "    print(\"\\n===== AGENT COMPARISON SUMMARY =====\")\n",
    "    print(\"\\nPerformance Metrics by Agent and Environment:\")\n",
    "    display_df = comparison_df.round(3)\n",
    "    print(display_df)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Average Reward\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Environment', y='Average Reward', hue='Agent', data=comparison_df, errorbar=('ci', 95))\n",
    "    plt.title('Average Reward by Agent and Environment')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimal Actions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Environment', y='Optimal Actions (%)', hue='Agent', data=comparison_df, errorbar=('ci', 95))\n",
    "    plt.title('Optimal Actions (%) by Agent and Environment')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Environment', y='Regret', hue='Agent', data=comparison_df, errorbar=('ci', 95))\n",
    "    plt.title('Regret by Agent and Environment')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Generate heatmap for average reward\n",
    "    plt.subplot(2, 2, 4)\n",
    "    reward_pivot = comparison_df.pivot_table(\n",
    "        index='Agent', \n",
    "        columns='Environment', \n",
    "        values='Average Reward'\n",
    "    )\n",
    "    sns.heatmap(reward_pivot, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "    plt.title('Average Reward Heatmap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Agent Performance Comparison\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate summary of best agent for each environment\n",
    "    print(\"\\nBest Agent for Each Environment (Based on Average Reward):\")\n",
    "    best_agents = comparison_df.loc[comparison_df.groupby('Environment')['Average Reward'].idxmax()]\n",
    "    print(best_agents[['Environment', 'Agent', 'Average Reward']].round(3))\n",
    "    \n",
    "    # Generate summary of best environment for each agent\n",
    "    print(\"\\nBest Environment for Each Agent (Based on Average Reward):\")\n",
    "    best_envs = comparison_df.loc[comparison_df.groupby('Agent')['Average Reward'].idxmax()]\n",
    "    print(best_envs[['Agent', 'Environment', 'Average Reward']].round(3))\n",
    "    \n",
    "    # Create simple ranking table based on average reward across all environments\n",
    "    print(\"\\nOverall Agent Ranking (Based on Average Reward Across All Environments):\")\n",
    "    agent_ranking = comparison_df.groupby('Agent')['Average Reward'].mean().reset_index()\n",
    "    agent_ranking = agent_ranking.sort_values('Average Reward', ascending=False)\n",
    "    print(agent_ranking.round(3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for students\n",
    "\"\"\"\n",
    "# STUDENTS: Implement your solutions for the agent functions above.\n",
    "# Then run the evaluations to test your implementations.\n",
    "\n",
    "# To evaluate just one agent type at a time (for optimization):\n",
    "# With fixed turns (T=100)\n",
    "full_info_results = evaluate_full_information_agent(fixed_turns=True)\n",
    "# OR\n",
    "partial_info_results = evaluate_partial_information_agent(fixed_turns=True)\n",
    "# OR\n",
    "reward_only_results = evaluate_reward_only_agent(fixed_turns=True)\n",
    "\n",
    "# With random turns (T=1-300)\n",
    "full_info_random_results = evaluate_full_information_agent(fixed_turns=False)\n",
    "# OR\n",
    "partial_info_random_results = evaluate_partial_information_agent(fixed_turns=False)\n",
    "# OR\n",
    "reward_only_random_results = evaluate_reward_only_agent(fixed_turns=False)\n",
    "\n",
    "# To run comprehensive evaluation of all agents:\n",
    "# With fixed turns (T=100)\n",
    "all_results = run_all_agents(fixed_turns=True)\n",
    "# With random turns (T=1-300)\n",
    "all_random_results = run_all_agents(fixed_turns=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentes para Definir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informacion Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "def full_information_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agente de Información Completa.\n",
    "    \n",
    "    Este agente conoce la probabilidad actual de recompensa del brazo 1 (visible)\n",
    "    y utiliza la historia de acciones y recompensas para estimar el rendimiento del brazo 2,\n",
    "    cuya probabilidad es oculta. La decisión se basa en comparar la probabilidad\n",
    "    actual del brazo 1 con la estimación empírica de la tasa de recompensa del brazo 2.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Diccionario que contiene:\n",
    "            - current_turn (int): Número de turno actual.\n",
    "            - total_turns (int): Número total de turnos en el juego.\n",
    "            - p1 (float): Probabilidad de recompensa del brazo 1 en el turno actual.\n",
    "            - history (Dict): Diccionario con la información histórica, que incluye:\n",
    "                - 'actions': Lista de acciones pasadas (0 para brazo 1, 1 para brazo 2).\n",
    "                - 'rewards': Lista de recompensas obtenidas en turnos anteriores.\n",
    "                - 'p1': Lista de probabilidades pasadas para el brazo 1.\n",
    "                (Nota: La probabilidad del brazo 2 no está incluida, pues es oculta.)\n",
    "    \n",
    "    Returns:\n",
    "        int: La acción a tomar (0 para el brazo 1, 1 para el brazo 2).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener la probabilidad actual del brazo 1\n",
    "    p1 = env_info.get('p1', 0.5)\n",
    "    \n",
    "    # Obtener el historial de acciones y recompensas\n",
    "    history = env_info.get('history', {})\n",
    "    actions = history.get('actions', [])\n",
    "    rewards = history.get('rewards', [])\n",
    "    \n",
    "    # Estimar la tasa de éxito empírica para el brazo 2 (acción 1)\n",
    "    arm2_rewards = [r for a, r in zip(actions, rewards) if a == 1]\n",
    "    if len(arm2_rewards) > 0:\n",
    "        p2_estimate = sum(arm2_rewards) / len(arm2_rewards)\n",
    "    else:\n",
    "        # Si aún no se ha seleccionado el brazo 2, se opta por explorarlo\n",
    "        return 1\n",
    "    \n",
    "    # Decisión: se elige el brazo con mayor probabilidad estimada de éxito\n",
    "    if p1 >= p2_estimate:\n",
    "        return 0  # Elegir brazo 1 (visible)\n",
    "    else:\n",
    "        return 1  # Elegir brazo 2 (oculto)\n",
    "    \n",
    "# Ejemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo de env_info con algunos datos históricos\n",
    "    env_info_example = {\n",
    "        'current_turn': 10,\n",
    "        'total_turns': 100,\n",
    "        'p1': 0.65,\n",
    "        'history': {\n",
    "            'actions': [0, 1, 0, 1],\n",
    "            'rewards': [1, 0, 1, 1],\n",
    "            'p1': [0.60, 0.62, 0.64, 0.66]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    action = full_information_agent(env_info_example)\n",
    "    print(\"Acción elegida:\", action)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_info_results = evaluate_full_information_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_info_results = evaluate_full_information_agent(fixed_turns=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informacion Parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_information_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agente de Información Parcial.\n",
    "    \n",
    "    Este agente solo tiene acceso a:\n",
    "      - La probabilidad actual de recompensa del brazo 1 (visible).\n",
    "      - El número de turno actual y el total de turnos.\n",
    "      - El historial de acciones y recompensas (para inferir el rendimiento del brazo 2).\n",
    "    \n",
    "    La estrategia implementada es una variante epsilon-greedy que:\n",
    "      - Estima la tasa de éxito empírica del brazo 2 a partir de su historial.\n",
    "      - Usa la probabilidad actual del brazo 1 directamente.\n",
    "      - Con una probabilidad epsilon (que disminuye a medida que avanza el juego) realiza exploración.\n",
    "      - Si no se explora, selecciona el brazo con la tasa de éxito (o probabilidad) mayor.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Diccionario que contiene:\n",
    "            - current_turn: Número de turno actual.\n",
    "            - total_turns: Número total de turnos.\n",
    "            - p1: Probabilidad actual del brazo 1.\n",
    "            - history: Diccionario con:\n",
    "                - 'actions': Lista de acciones pasadas (0 para brazo 1, 1 para brazo 2).\n",
    "                - 'rewards': Lista de recompensas obtenidas en turnos anteriores.\n",
    "    \n",
    "    Returns:\n",
    "        int: La acción a tomar (0 para el brazo 1, 1 para el brazo 2).\n",
    "    \"\"\"\n",
    "    current_turn = env_info.get('current_turn', 0)\n",
    "    total_turns = env_info.get('total_turns', 100)\n",
    "    p1 = env_info.get('p1', 0.5)\n",
    "    history = env_info.get('history', {})\n",
    "    \n",
    "    # Extraer historial de acciones y recompensas\n",
    "    actions = history.get('actions', [])\n",
    "    rewards = history.get('rewards', [])\n",
    "    \n",
    "    # Calcular la tasa de éxito empírica para el brazo 2, usando solo los turnos en los que se jugó ese brazo.\n",
    "    arm2_rewards = [r for a, r in zip(actions, rewards) if a == 1]\n",
    "    if len(arm2_rewards) > 0:\n",
    "        arm2_mean = sum(arm2_rewards) / len(arm2_rewards)\n",
    "    else:\n",
    "        # Si el brazo 2 aún no ha sido explorado, usar un valor neutro.\n",
    "        arm2_mean = 0.5\n",
    "    \n",
    "    # Definir un parámetro epsilon para exploración, que disminuye conforme avanzan los turnos.\n",
    "    # Por ejemplo, epsilon se puede definir de modo lineal, con un mínimo de 0.1.\n",
    "    epsilon = max(0.1, 1.0 - (current_turn / total_turns))\n",
    "    \n",
    "    # Con probabilidad epsilon, se realiza una acción aleatoria para explorar.\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, 2)\n",
    "    \n",
    "    # En caso de explotación, se compara la probabilidad visible del brazo 1 (p1)\n",
    "    # con la tasa de éxito estimada del brazo 2 (arm2_mean).\n",
    "    if p1 >= arm2_mean:\n",
    "        return 0  # Seleccionar el brazo 1\n",
    "    else:\n",
    "        return 1  # Seleccionar el brazo 2\n",
    "\n",
    "# Ejemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo de env_info\n",
    "    env_info_example = {\n",
    "        'current_turn': 20,\n",
    "        'total_turns': 100,\n",
    "        'p1': 0.55,\n",
    "        'history': {\n",
    "            'actions': [0, 1, 0, 0, 1],\n",
    "            'rewards': [1, 0, 1, 1, 1]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    action = partial_information_agent(env_info_example)\n",
    "    print(\"Acción elegida:\", action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_info_results = evaluate_partial_information_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_info_results = evaluate_partial_information_agent(fixed_turns=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "def reward_only_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent that only sees the rewards but not the number of turns T.\n",
    "    \n",
    "    This agent does not have access to any probability information or the total number of turns.\n",
    "    Instead, it must base its decision solely on the observed rewards in its history.\n",
    "    \n",
    "    The strategy implemented here is a simple greedy one:\n",
    "      - Compute the empirical average reward for each arm (0 and 1) based on the history.\n",
    "      - If an arm has not been played yet, select it to gather information.\n",
    "      - Otherwise, choose the arm with the higher average reward.\n",
    "      \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn: Current turn number.\n",
    "            - history: Dictionary with past actions and rewards:\n",
    "                - 'actions': List[int] of past actions (0 for arm 1, 1 for arm 2).\n",
    "                - 'rewards': List[float] of rewards received.\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2).\n",
    "    \"\"\"\n",
    "    # Extract the history information\n",
    "    history = env_info.get('history', {})\n",
    "    actions = history.get('actions', [])\n",
    "    rewards = history.get('rewards', [])\n",
    "    \n",
    "    # If no actions have been taken, choose randomly (or default to one arm)\n",
    "    if not actions:\n",
    "        return np.random.randint(0, 2)\n",
    "    \n",
    "    # Compute empirical averages for each arm\n",
    "    sum_rewards = {0: 0.0, 1: 0.0}\n",
    "    count_rewards = {0: 0, 1: 0}\n",
    "    \n",
    "    for action, reward in zip(actions, rewards):\n",
    "        sum_rewards[action] += reward\n",
    "        count_rewards[action] += 1\n",
    "    \n",
    "    # If an arm has not been played yet, select it to ensure exploration\n",
    "    if count_rewards[0] == 0:\n",
    "        return 0\n",
    "    if count_rewards[1] == 0:\n",
    "        return 1\n",
    "    \n",
    "    avg_reward_0 = sum_rewards[0] / count_rewards[0]\n",
    "    avg_reward_1 = sum_rewards[1] / count_rewards[1]\n",
    "    \n",
    "    # Greedy decision: select the arm with the higher average reward\n",
    "    if avg_reward_0 >= avg_reward_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example env_info with a history of actions and rewards\n",
    "    env_info_example = {\n",
    "        'current_turn': 15,\n",
    "        'history': {\n",
    "            'actions': [0, 1, 1, 0, 0],\n",
    "            'rewards': [1, 0, 1, 1, 0]\n",
    "        }\n",
    "    }\n",
    "    chosen_action = reward_only_agent(env_info_example)\n",
    "    print(\"Chosen action:\", chosen_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_only_results = evaluate_reward_only_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_only_results = evaluate_reward_only_agent(fixed_turns=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
